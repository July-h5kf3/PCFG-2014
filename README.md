# PCFG-2014
09版本的改进，不采用外部导入字典，且结果呈现方式采取概率阀值图
#优化后的PCFG猜测模型
*****
## reference
[A Study of Probabilistic Password Models](https://ieeexplore.ieee.org/document/6956595)

## 改良前的PCFG猜测模型大致思路
将密码依据字符类型划分例如$password123！$被划分为$L_{8}D_{3}S_{1}$,然后分别统计出每种类型的出现概率，以及字段出现的概率，最后算出整个字符串的概率。（如$D_{3}$有3种，则$D_{3}->123$的概率则为$\frac{1}{3}$）
## 改进之处
 $1.$ 不再采用外部导入的字典而是通过训练集生成一个字典
 
 $2.$ 采用了一种新的密码生成方式而非之前的优先队列模式
## 新的密码生成方式：阈值搜索算法
类似于迭代加深状态空间搜索方法，它对搜索树进行多次深度优先遍历。在第$i$次迭代中，通过对树进行深度优先遍历，剪枝所有概率小于$ρi$的节点，并输出概率在目标范围内的所有密码，生成概率在$(ρ_{i}, τ_{i}]$范围内的密码。为了生成$n$个密码猜测，我们从保守的范围$(ρ_{1} = \frac{1}{n}，τ_{1} = 1]$开始。在第$i$次迭代后，如果到目前为止生成的密码数为$m<n$，我们将以$(ρ_{i+1} = \frac{ρ_{i}}{min(2,1.5\frac{n}{m})}，τ_{i+1} = ρ_{i}]$开始另一次迭代。也就是说，当$m < 0.75n$时，我们将概率阈值减半。我们经验上观察到，将$ρ$减半会导致生成的密码数量接近翻倍。我们可能会超过$n$生成更多的密码，但极不可能生成超过$2n$的猜测。该算法的平均运行复杂度为$O(n)$，即与n线性关系。内存复杂度（不包括模型数据或生成的密码）基本上是常数，因为唯一需要的数据结构是容量为$U +1$的栈

需要注意的是，这里的搜索树是指在原本方法中由高概率预终端结构A通过替换的方式生成低概率预终端结构B形成的树状结构

## 预处理
本方法中的预处理与先前的方式不同。先前的版本中不考虑大小写带来的差异而简单的将字母部分当做L，而改良后的版本则采取用$\alpha$来代表大写字母段，而$\beta$来代表小写字母段

### step $1$ 数据处理
根据论文，删去了包含任何超出95个可以打印ASCLL符号的字符的密码，以及密码长度大于40或者小于4的密码

### step $2$ 结构预处理
小写字母段落记为$\beta$,大写字母段记为$\alpha$,由于编译器中无希腊字母因此用大写字母$A$,$B$替代，数字段落用$D$表示，而特殊字符段用$S$表示从而得到$base-struct$的类型以及对应的概率。

### step $3$ grammer 生成
本部分中$D$以及$L$部分的概率统计以及语法规则的生成与先前一致，而对于$\alpha$和$\beta$的部分并不统计概率，而是生成字典。特别地，要对生成地规律按概率大小从大到小排序

考虑将字典生成以及$D$,$S$的语法生成通过两个程序实现
## 口令生成
### $pre-terminal-struct$的生成
对于每个$base-struct$通过对$D$与$S$部分的替换产生搜索树的第二层(根节点为空节点),需要注意的是计算$pre-terminal-struct$的概率时要考虑$\alpha$和$\beta$的概率。后续则根据阈值搜索算法生成其他的$pre-terminal-struct$
### $terminal - struct$的生成（attack）
利用深度搜索算法将$pre - terminal- strcut$中的$\alpha$,$\beta$部分进行替换

## 图像生成
利用训练集产生的PCFG文法，计算测试集中每个口令的概率，然后生成概率阀值图
